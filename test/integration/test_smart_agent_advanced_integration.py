#!/usr/bin/env python3
"""
–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã Smart Agent
–¢–µ—Å—Ç–∏—Ä—É–µ—Ç fallback —Ä–µ–∂–∏–º—ã, best_of_two —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∏ —Å–ª–æ–∂–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
"""

import sys
import os
import tempfile
import shutil
import time
import json
from pathlib import Path
from unittest.mock import patch, MagicMock, AsyncMock
import pytest
import asyncio

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import yaml


class TestSmartAgentAdvancedIntegration:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã Smart Agent"""

    def setup_method(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º —Ç–µ—Å—Ç–æ–º"""
        self.temp_dir = Path(tempfile.mkdtemp(prefix="smart_agent_advanced_test_"))
        self.experience_dir = self.temp_dir / "experience"
        self.project_dir = self.temp_dir / "project"

        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞
        self.project_dir.mkdir()

        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞
        (self.project_dir / "main.py").write_text("""
import os
from pathlib import Path
from utils.helper import helper_function

def main():
    result = helper_function()
    print(f"Result: {result}")
    return result

if __name__ == "__main__":
    main()
""")

        (self.project_dir / "utils.py").write_text("""
def helper_function():
    return "test result"

class TestClass:
    def __init__(self, value):
        self.value = value

    def process(self):
        return f"processed: {self.value}"
""")

        (self.project_dir / "requirements.txt").write_text("""
pytest>=7.0.0
requests>=2.28.0
""")

    def teardown_method(self):
        """–û—á–∏—Å—Ç–∫–∞ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Å—Ç–∞"""
        if self.temp_dir.exists():
            shutil.rmtree(self.temp_dir)

    def test_learning_tool_context_analyzer_integration_workflow(self):
        """–¢–µ—Å—Ç –ø–æ–ª–Ω–æ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞: –∞–Ω–∞–ª–∏–∑ -> –æ–±—É—á–µ–Ω–∏–µ -> —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è"""
        from src.tools.learning_tool import LearningTool
        from src.tools.context_analyzer_tool import ContextAnalyzerTool

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
        learning_tool = LearningTool(experience_dir=str(self.experience_dir))
        context_tool = ContextAnalyzerTool(project_dir=str(self.project_dir))

        # –®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–µ–∫—Ç
        project_analysis = context_tool._run("analyze_project")
        assert "main.py" in project_analysis
        assert "utils.py" in project_analysis

        # –®–∞–≥ 2: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ–∞–π–ª
        file_analysis = context_tool._run("analyze_file", file_path="main.py")
        assert "helper_function" in file_analysis
        assert "import" in file_analysis

        # –®–∞–≥ 3: –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—ã—Ç –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–µ–∫—Ç–∞
        learning_tool._run("save_experience",
                          task_id="project_analysis_001",
                          task_description="–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã Python –ø—Ä–æ–µ–∫—Ç–∞",
                          success=True,
                          execution_time=2.5,
                          patterns=["project_analysis", "python_structure"],
                          notes="–ù–∞–π–¥–µ–Ω—ã main.py, utils.py, requirements.txt")

        # –®–∞–≥ 4: –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—ã—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞
        learning_tool._run("save_experience",
                          task_id="file_analysis_002",
                          task_description="–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π Python —Ñ–∞–π–ª–∞",
                          success=True,
                          execution_time=1.2,
                          patterns=["dependency_analysis", "python_imports"],
                          notes="–ù–∞–π–¥–µ–Ω—ã –∏–º–ø–æ—Ä—Ç—ã pathlib, os, utils.helper")

        # –®–∞–≥ 5: –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø–æ—Ö–æ–∂–µ–π –∑–∞–¥–∞—á–∏
        recommendations = learning_tool._run("get_recommendations",
                                           current_task="–∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–æ–≤–æ–≥–æ Python –ø—Ä–æ–µ–∫—Ç–∞")

        assert "project_analysis_001" in recommendations
        assert "python_structure" in recommendations

        # –®–∞–≥ 6: –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –∑–∞–¥–∞—á–∏
        similar_tasks = learning_tool._run("find_similar_tasks",
                                         query="–∞–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Ñ–∞–π–ª–∞")

        assert "file_analysis_002" in similar_tasks
        assert "dependency_analysis" in similar_tasks

    def test_fallback_mode_integration(self):
        """–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ fallback —Ä–µ–∂–∏–º–∞ –ø—Ä–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ LLM"""
        from src.agents.smart_agent import create_smart_agent

        # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞ –±–µ–∑ LLM (fallback —Ä–µ–∂–∏–º)
        agent = create_smart_agent(
            project_dir=self.project_dir,
            experience_dir="experience",
            use_llm=False,  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º LLM
            verbose=False
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∞–≥–µ–Ω—Ç —Å–æ–∑–¥–∞–ª—Å—è –±–µ–∑ LLM
        assert agent.llm is None
        assert len(agent.tools) >= 2  # –î–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å LearningTool –∏ ContextAnalyzerTool

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
        tool_names = [tool.__class__.__name__ for tool in agent.tools]
        assert "LearningTool" in tool_names
        assert "ContextAnalyzerTool" in tool_names

    @patch('src.llm.llm_manager.AsyncOpenAI')
    def test_best_of_two_strategy_integration(self, mock_openai_class):
        """–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ best_of_two —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        from src.llm.llm_manager import LLMManager

        # –°–æ–∑–¥–∞–µ–º –º–æ–∫ –∫–ª–∏–µ–Ω—Ç–∞
        mock_client = AsyncMock()
        mock_openai_class.return_value = mock_client

        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–µ–π
        async def mock_call(*args, **kwargs):
            model_name = kwargs.get('model', 'test-model')
            if 'model1' in model_name:
                return MagicMock(
                    choices=[MagicMock(message=MagicMock(content='Response from model1'))],
                    usage=MagicMock(prompt_tokens=10, completion_tokens=20)
                )
            elif 'model2' in model_name:
                return MagicMock(
                    choices=[MagicMock(message=MagicMock(content='Response from model2'))],
                    usage=MagicMock(prompt_tokens=15, completion_tokens=25)
                )
            else:
                return MagicMock(
                    choices=[MagicMock(message=MagicMock(content='Evaluator response: 0.8'))],
                    usage=MagicMock(prompt_tokens=5, completion_tokens=10)
                )

        mock_client.chat.completions.create = mock_call

        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config_path = self.temp_dir / "test_llm_config.yaml"
        config_data = {
            'llm': {
                'default_provider': 'test_provider',
                'strategy': 'best_of_two',
                'parallel': {
                    'models': ['test-model1', 'test-model2'],
                    'evaluator_model': 'test-evaluator'
                }
            },
            'providers': {
                'test_provider': {
                    'base_url': 'https://test.api',
                    'models': [
                        {'name': 'test-model1', 'max_tokens': 1000, 'context_window': 4000},
                        {'name': 'test-model2', 'max_tokens': 1000, 'context_window': 4000},
                        {'name': 'test-evaluator', 'max_tokens': 500, 'context_window': 2000}
                    ]
                }
            }
        }

        with open(config_path, 'w') as f:
            yaml.dump(config_data, f)

        # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä LLM
        manager = LLMManager(config_path=str(config_path))

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
        async def test_parallel():
            response = await manager.generate_response(
                prompt="Test prompt",
                use_parallel=True
            )

            assert response.success
            assert "Response from" in response.content

        asyncio.run(test_parallel())

    @patch('src.llm.llm_manager.AsyncOpenAI')
    def test_parallel_fallback_when_one_model_fails(self, mock_openai_class):
        """–¢–µ—Å—Ç fallback –≤ best_of_two –∫–æ–≥–¥–∞ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –ø–∞–¥–∞–µ—Ç"""
        from src.llm.llm_manager import LLMManager

        # –°–æ–∑–¥–∞–µ–º –º–æ–∫ –∫–ª–∏–µ–Ω—Ç–∞
        mock_client = AsyncMock()
        mock_openai_class.return_value = mock_client

        call_count = 0

        async def mock_call(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            model_name = kwargs.get('model', 'test-model')

            if 'model1' in model_name:
                # –ü–µ—Ä–≤–∞—è –º–æ–¥–µ–ª—å –ø–∞–¥–∞–µ—Ç
                raise Exception("Model 1 failed")
            elif 'model2' in model_name:
                return MagicMock(
                    choices=[MagicMock(message=MagicMock(content='Response from model2 (fallback)'))],
                    usage=MagicMock(prompt_tokens=15, completion_tokens=25)
                )
            else:
                return MagicMock(
                    choices=[MagicMock(message=MagicMock(content='Evaluator response: 0.9'))],
                    usage=MagicMock(prompt_tokens=5, completion_tokens=10)
                )

        mock_client.chat.completions.create = mock_call

        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config_path = self.temp_dir / "test_llm_config.yaml"
        config_data = {
            'llm': {
                'default_provider': 'test_provider',
                'strategy': 'best_of_two',
                'parallel': {
                    'models': ['test-model1', 'test-model2'],
                    'evaluator_model': 'test-evaluator'
                }
            },
            'providers': {
                'test_provider': {
                    'base_url': 'https://test.api',
                    'models': [
                        {'name': 'test-model1', 'max_tokens': 1000, 'context_window': 4000},
                        {'name': 'test-model2', 'max_tokens': 1000, 'context_window': 4000},
                        {'name': 'test-evaluator', 'max_tokens': 500, 'context_window': 2000}
                    ]
                }
            }
        }

        with open(config_path, 'w') as f:
            yaml.dump(config_data, f)

        # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä LLM
        manager = LLMManager(config_path=str(config_path))

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å fallback
        async def test_parallel_fallback():
            response = await manager.generate_response(
                prompt="Test prompt with failure",
                use_parallel=True
            )

            assert response.success
            assert "Response from model2 (fallback)" in response.content

        asyncio.run(test_parallel_fallback())

    def test_smart_agent_with_real_tools_only(self):
        """–¢–µ—Å—Ç Smart Agent –≤ —Ä–µ–∂–∏–º–µ —Ç–æ–ª—å–∫–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ (–±–µ–∑ LLM)"""
        from src.agents.smart_agent import create_smart_agent
        from src.tools.learning_tool import LearningTool
        from src.tools.context_analyzer_tool import ContextAnalyzerTool

        # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞ –±–µ–∑ LLM
        agent = create_smart_agent(
            project_dir=self.project_dir,
            experience_dir="experience",
            use_llm=False,
            verbose=False
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
        learning_tool = None
        context_tool = None

        for tool in agent.tools:
            if isinstance(tool, LearningTool):
                learning_tool = tool
            elif isinstance(tool, ContextAnalyzerTool):
                context_tool = tool

        assert learning_tool is not None
        assert context_tool is not None

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–±–æ—Ç—É –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç–∞
        # (–í —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∞–≥–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª –±—ã LLM –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∫–æ–º–∞–Ω–¥,
        # –Ω–æ –≤ tool-only —Ä–µ–∂–∏–º–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞–ø—Ä—è–º—É—é)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—ã—Ç —á–µ—Ä–µ–∑ LearningTool
        result = learning_tool._run("save_experience",
                                   task_id="tool_only_test_001",
                                   task_description="–¢–µ—Å—Ç —Ä–∞–±–æ—Ç—ã –≤ tool-only —Ä–µ–∂–∏–º–µ",
                                   success=True,
                                   execution_time=1.0,
                                   patterns=["tool_only", "integration_test"],
                                   notes="–¢–µ—Å—Ç –±–µ–∑ LLM")

        assert "—Å–æ—Ö—Ä–∞–Ω–µ–Ω" in result.lower()

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–µ–∫—Ç —á–µ—Ä–µ–∑ ContextAnalyzerTool
        analysis = context_tool._run("analyze_project")
        assert "main.py" in analysis
        assert "utils.py" in analysis

        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –∑–∞–¥–∞—á–∏
        similar = learning_tool._run("find_similar_tasks",
                                    query="tool-only —Ä–µ–∂–∏–º")

        assert "tool_only_test_001" in similar

    def test_memory_management_with_large_experience(self):
        """–¢–µ—Å—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é –ø—Ä–∏ –±–æ–ª—å—à–æ–º –æ–±—ä–µ–º–µ –æ–ø—ã—Ç–∞"""
        from src.tools.learning_tool import LearningTool

        # –°–æ–∑–¥–∞–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
        tool = LearningTool(
            experience_dir=str(self.experience_dir),
            max_experience_tasks=50  # –ú–∞–ª–µ–Ω—å–∫–∏–π –ª–∏–º–∏—Ç –¥–ª—è —Ç–µ—Å—Ç–∞
        )

        # –î–æ–±–∞–≤–ª—è–µ–º 100 –∑–∞–¥–∞—á (–±–æ–ª—å—à–µ –ª–∏–º–∏—Ç–∞)
        for i in range(100):
            tool._run("save_experience",
                     task_id=f"memory_test_{i:03d}",
                     task_description=f"–ó–∞–¥–∞—á–∞ –¥–ª—è —Ç–µ—Å—Ç–∞ –ø–∞–º—è—Ç–∏ #{i}",
                     success=True,
                     execution_time=1.0,
                     patterns=[f"pattern_{i%10}"],
                     notes=f"–¢–µ—Å—Ç–æ–≤–∞—è –∑–∞–º–µ—Ç–∫–∞ #{i}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –Ω–µ –±–æ–ª—å—à–µ max_experience_tasks
        experience_file = self.experience_dir / "experience.json"
        with open(experience_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        assert len(data['tasks']) <= 50

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Å—Ç–∞–ª–∏—Å—å —Å–∞–º—ã–µ —Å–≤–µ–∂–∏–µ –∑–∞–¥–∞—á–∏
        task_ids = [t['task_id'] for t in data['tasks']]
        # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 50 –∑–∞–¥–∞—á –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã
        expected_ids = [f"memory_test_{i:03d}" for i in range(50, 100)]
        for expected_id in expected_ids:
            assert expected_id in task_ids

    def test_error_recovery_integration(self):
        """–¢–µ—Å—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ—à–∏–±–æ–∫ –≤ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏"""
        from src.tools.learning_tool import LearningTool
        from src.tools.context_analyzer_tool import ContextAnalyzerTool

        learning_tool = LearningTool(experience_dir=str(self.experience_dir))
        context_tool = ContextAnalyzerTool(project_dir=str(self.project_dir))

        # –¢–µ—Å—Ç 1: –û—à–∏–±–∫–∞ –≤ LearningTool –Ω–µ –¥–æ–ª–∂–Ω–∞ –ª–æ–º–∞—Ç—å ContextAnalyzerTool
        try:
            learning_tool._run("save_experience",
                             task_id="",  # –ü—É—Å—Ç–æ–π ID –≤—ã–∑–æ–≤–µ—Ç –æ—à–∏–±–∫—É
                             task_description="–¢–µ—Å—Ç –æ—à–∏–±–∫–∏",
                             success=True)
            assert False, "–î–æ–ª–∂–Ω–∞ –±—ã–ª–∞ –≤–æ–∑–Ω–∏–∫–Ω—É—Ç—å –æ—à–∏–±–∫–∞"
        except (ValueError, AssertionError):
            pass  # –û–∂–∏–¥–∞–µ–º–∞—è –æ—à–∏–±–∫–∞

        # ContextAnalyzerTool –¥–æ–ª–∂–µ–Ω –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Ä–∞–±–æ—Ç–∞—Ç—å
        analysis = context_tool._run("analyze_project")
        assert len(analysis) > 0
        assert "main.py" in analysis

        # –¢–µ—Å—Ç 2: –û—à–∏–±–∫–∞ –≤ ContextAnalyzerTool –Ω–µ –¥–æ–ª–∂–Ω–∞ –ª–æ–º–∞—Ç—å LearningTool
        try:
            context_tool._run("analyze_file",
                            file_path="nonexistent_file.py")
            assert False, "–î–æ–ª–∂–Ω–∞ –±—ã–ª–∞ –≤–æ–∑–Ω–∏–∫–Ω—É—Ç—å –æ—à–∏–±–∫–∞"
        except (FileNotFoundError, ValueError):
            pass  # –û–∂–∏–¥–∞–µ–º–∞—è –æ—à–∏–±–∫–∞

        # LearningTool –¥–æ–ª–∂–µ–Ω –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Ä–∞–±–æ—Ç–∞—Ç—å
        result = learning_tool._run("save_experience",
                                   task_id="recovery_test_001",
                                   task_description="–¢–µ—Å—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏",
                                   success=True,
                                   execution_time=1.0,
                                   patterns=["error_recovery", "integration"],
                                   notes="–¢–µ—Å—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è")

        assert "—Å–æ—Ö—Ä–∞–Ω–µ–Ω" in result.lower()


def run_advanced_integration_tests():
    """–ó–∞–ø—É—Å–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤"""
    print("üöÄ –ó–∞–ø—É—Å–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤ Smart Agent...")

    test_instance = TestSmartAgentAdvancedIntegration()

    tests = [
        ("LearningTool + ContextAnalyzerTool Workflow", test_instance.test_learning_tool_context_analyzer_integration_workflow),
        ("Fallback Mode Integration", test_instance.test_fallback_mode_integration),
        ("Best-of-Two Strategy Integration", test_instance.test_best_of_two_strategy_integration),
        ("Parallel Fallback When One Model Fails", test_instance.test_parallel_fallback_when_one_model_fails),
        ("Smart Agent Tools-Only Mode", test_instance.test_smart_agent_with_real_tools_only),
        ("Memory Management with Large Experience", test_instance.test_memory_management_with_large_experience),
        ("Error Recovery Integration", test_instance.test_error_recovery_integration),
    ]

    results = []

    for test_name, test_func in tests:
        try:
            print(f"\nüîß –ó–∞–ø—É—Å–∫: {test_name}")
            test_instance.setup_method()
            test_func()
            test_instance.teardown_method()
            results.append((test_name, True))
            print(f"‚úÖ {test_name}: –ü–†–û–ô–î–ï–ù")
        except Exception as e:
            test_instance.teardown_method()
            results.append((test_name, False))
            print(f"‚ùå {test_name}: –ü–†–û–í–ê–õ–ï–ù - {e}")

    # –ò—Ç–æ–≥–∏
    print("\n" + "="*80)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –†–ê–°–®–ò–†–ï–ù–ù–´–• –ò–ù–¢–ï–ì–†–ê–¶–ò–û–ù–ù–´–• –¢–ï–°–¢–û–í SMART AGENT")
    print("="*80)

    passed = 0
    total = len(results)

    for test_name, success in results:
        status = "‚úÖ –ü–†–û–ô–î–ï–ù" if success else "‚ùå –ü–†–û–í–ê–õ–ï–ù"
        print("40")

        if success:
            passed += 1

    print(f"\nüìà –ò–¢–û–ì–û: {passed}/{total} —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–π–¥–µ–Ω–æ")

    if passed == total:
        print("üéâ –í–°–ï –†–ê–°–®–ò–†–ï–ù–ù–´–ï –ò–ù–¢–ï–ì–†–ê–¶–ò–û–ù–ù–´–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´!")
        return 0
    else:
        print("‚ö†Ô∏è  –ù–ï–ö–û–¢–û–†–´–ï –¢–ï–°–¢–´ –ü–†–û–í–ê–õ–ï–ù–´.")
        return 1


if __name__ == "__main__":
    sys.exit(run_advanced_integration_tests())