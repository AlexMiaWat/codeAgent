"""
LearningTool - –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö
"""

import json
import logging
import unicodedata
try:
    import fcntl
    HAS_FCNTL = True
except ImportError:
    fcntl = None # type: ignore
    HAS_FCNTL = False
import os
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Set
from datetime import datetime, timedelta
import hashlib
from crewai.tools.base_tool import BaseTool

logger = logging.getLogger(__name__)


def normalize_unicode_text(text: Optional[str]) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç Unicode —Ç–µ–∫—Å—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç

    Returns:
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if text is None:
        return ""
    if not text:
        return text

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º Unicode (NFD - canonical decomposition)
    normalized = unicodedata.normalize('NFD', text)

    # –£–¥–∞–ª—è–µ–º –¥–∏–∞–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞–∫–∏ (combining characters)
    normalized = ''.join(char for char in normalized if unicodedata.category(char) != 'Mn')

    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    normalized = normalized.lower()

    return normalized


class LearningTool(BaseTool):
    """
    –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö –∏ –æ–ø—ã—Ç–µ.

    –ü–æ–∑–≤–æ–ª—è–µ—Ç:
    - –°–æ—Ö—Ä–∞–Ω—è—Ç—å –æ–ø—ã—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á
    - –ò—Å–∫–∞—Ç—å –ø–æ—Ö–æ–∂–∏–µ –∑–∞–¥–∞—á–∏ –≤ –∏—Å—Ç–æ—Ä–∏–∏
    - –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ–ø—ã—Ç–∞
    """

    name: str = "LearningTool"
    description: str = """
    –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö.
    –ü–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –æ–ø—ã—Ç, –∏—Å–∫–∞—Ç—å –ø–æ—Ö–æ–∂–∏–µ –∑–∞–¥–∞—á–∏ –∏ –¥–∞–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.
    """
    experience_dir: str = "smart_experience"
    max_experience_tasks: int = 1000
    experience_file: str = "experience.json"
    cache_file: str = "cache.json"
    lock_file: str = ""


    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
    enable_indexing: bool = True
    cache_size: int = 1000
    cache_ttl_seconds: int = 3600  # 1 —á–∞—Å
    enable_cache_persistence: bool = False  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫—ç—à –Ω–∞ –¥–∏—Å–∫

    def __init__(self, experience_dir: str = "smart_experience", max_experience_tasks: int = 1000,
                 enable_indexing: bool = True, cache_size: int = 1000, cache_ttl_seconds: int = 3600,
                 enable_cache_persistence: bool = False, **kwargs):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LearningTool

        Args:
            experience_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–ø—ã—Ç–∞
            max_experience_tasks: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á –≤ –æ–ø—ã—Ç–µ
            enable_indexing: –í–∫–ª—é—á–∏—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            cache_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
            cache_ttl_seconds: –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫—ç—à–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            enable_cache_persistence: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫—ç—à –Ω–∞ –¥–∏—Å–∫ –¥–ª—è persistence –º–µ–∂–¥—É –∑–∞–ø—É—Å–∫–∞–º–∏
        """
        super().__init__(**kwargs)
        self.experience_dir = Path(experience_dir)
        self.max_experience_tasks = max_experience_tasks
        self.enable_indexing = enable_indexing
        self.cache_size = cache_size
        self.cache_ttl_seconds = cache_ttl_seconds
        self.enable_cache_persistence = enable_cache_persistence
        self.experience_file = self.experience_dir / "experience.json"

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –æ–ø—ã—Ç–∞ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        self.experience_dir.mkdir(parents=True, exist_ok=True)
        self.lock_file = str(self.experience_dir / (self.experience_file.name + ".lock"))
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
        self._search_index: Dict[str, Set[str]] = {}
        self._pattern_index: Dict[str, List[str]] = {}
        self._query_cache: Dict[str, Dict[str, Any]] = {}  # {query_hash: {result, timestamp}}
        self._cache_stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
            'size': 0
        }

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª –æ–ø—ã—Ç–∞ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if not self.experience_file.exists():
            self._init_experience_file()
        else:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –æ–ø—ã—Ç –∏ —Å—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å—ã
            self._load_and_index_experience()

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–π –∫—ç—à –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        if self.enable_cache_persistence:
            self._load_persistent_cache()

    def _init_experience_file(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–∞ –æ–ø—ã—Ç–∞"""
        initial_data = {
            "version": "1.0",
            "tasks": [],
            "patterns": {},
            "statistics": {
                "total_tasks": 0,
                "successful_tasks": 0,
                "failed_tasks": 0,
                "average_execution_time": 0
            }
        }

        with open(self.experience_file, 'w', encoding='utf-8') as f:
            json.dump(initial_data, f, indent=2, ensure_ascii=False)

    def _load_and_index_experience(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–ø—ã—Ç–∞ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        data = self._load_experience()
        tasks = data.get("tasks", [])

        if self.enable_indexing:
            # –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Å–ª–æ–≤–∞–º
            self._search_index = {}
            self._pattern_index = {}

            for task in tasks:
                task_id = task.get("task_id", "")
                description = normalize_unicode_text(task.get("description", ""))
                patterns = task.get("patterns", [])

                # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Å–ª–æ–≤–∞ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
                words = set(description.split())
                for word in words:
                    if word not in self._search_index:
                        self._search_index[word] = set()
                    self._search_index[word].add(task_id)

                # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
                for pattern in patterns:
                    if pattern not in self._pattern_index:
                        self._pattern_index[pattern] = []
                    self._pattern_index[pattern].append(task_id)

            logger.debug(f"Built search index with {len(self._search_index)} words and {len(self._pattern_index)} patterns")

    def _load_experience(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ–ø—ã—Ç–∞"""
        try:
                with open(self.experience_file, 'r', encoding='utf-8') as f:
                    if HAS_FCNTL:
                        fcntl.flock(f.fileno(), fcntl.LOCK_SH)  # Shared lock for reading
                        data = json.load(f)
                        fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                    else:
                        # Fallback for systems without fcntl (e.g., Windows)
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
                        # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –∑–¥–µ—Å—å –Ω—É–∂–Ω–∞ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                        # –î–ª—è smoke-—Ç–µ—Å—Ç–æ–≤ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ –∑–∞–¥–µ—Ä–∂–∫–∏
                        retries = 5
                        lock_path = Path(self.lock_file)
                        while lock_path.exists() and retries > 0:
                            time.sleep(0.05)
                            retries -= 1
                        if lock_path.exists():
                            logger.warning(f"Could not acquire lock for reading {self.experience_file}. Proceeding without lock.")
                        data = json.load(f)
                return data
        except Exception as e:
            logger.error(f"Failed to load experience from {self.experience_file}: {e}", exc_info=True)
            return {"version": "1.0", "tasks": [], "patterns": {}, "statistics": {}}

    def _save_experience(self, data: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–ø—ã—Ç–∞"""
        try:
            with open(self.experience_file, 'w', encoding='utf-8') as f:
                if HAS_FCNTL:
                    fcntl.flock(f.fileno(), fcntl.LOCK_EX)  # Exclusive lock for writing
                    json.dump(data, f, indent=2, ensure_ascii=False)
                    fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                else:
                    # Fallback for systems without fcntl (e.g., Windows)
                    lock_path = Path(self.lock_file)
                    lock_path.touch()
                    json.dump(data, f, indent=2, ensure_ascii=False)
                    if lock_path.exists():
                        lock_path.unlink() # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –ø–æ—Å–ª–µ –∑–∞–ø–∏—Å–∏
        except Exception as e:
            logger.error(f"Failed to save experience to {self.experience_file}: {e}", exc_info=True)

    def _run(self, action: str, **kwargs) -> str:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –æ–±—É—á–µ–Ω–∏—è

        Args:
            action: –î–µ–π—Å—Ç–≤–∏–µ (save_experience, find_similar, get_recommendations)
            **kwargs: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ–π—Å—Ç–≤–∏—è

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏—è
        """
        try:
            if action == "save_experience":
                return self.save_task_experience(**kwargs)
            elif action == "find_similar":
                return self.find_similar_tasks(**kwargs)
            elif action == "get_recommendations":
                return self.get_recommendations(**kwargs)
            elif action == "get_statistics":
                return self.get_statistics()
            else:
                return f"Unknown action: {action}"

        except Exception as e:
            logger.error(f"Error in LearningTool._run with action '{action}': {e}", exc_info=True)
            return f"Error executing action '{action}': {str(e)}"

    def save_task_experience(self, task_id: str, task_description: str,
                           success: bool, execution_time: Optional[float] = None,
                           notes: str = "", patterns: List[str] = None) -> str:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–ø—ã—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏

        Args:
            task_id: ID –∑–∞–¥–∞—á–∏
            task_description: –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
            success: –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            execution_time: –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            notes: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏
            patterns: –ü–∞—Ç—Ç–µ—Ä–Ω—ã/—à–∞–±–ª–æ–Ω—ã –∑–∞–¥–∞—á–∏

        Returns:
            –°–æ–æ–±—â–µ–Ω–∏–µ –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏
        """
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if not task_id or not isinstance(task_id, str):
            raise ValueError("task_id must be a non-empty string")

        if not task_description or not isinstance(task_description, str):
            raise ValueError("task_description must be a non-empty string")

        if not isinstance(success, bool):
            raise ValueError("success must be a boolean")

        if execution_time is not None and (not isinstance(execution_time, (int, float)) or execution_time < 0):
            raise ValueError("execution_time must be a non-negative number or None")

        if patterns is not None and not isinstance(patterns, list):
            raise ValueError("patterns must be a list or None")

        if patterns is not None:
            for pattern in patterns:
                if not isinstance(pattern, str):
                    raise ValueError("all patterns must be strings")

        data = self._load_experience()

        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –æ –∑–∞–¥–∞—á–µ
        task_record = {
            "task_id": task_id,
            "description": task_description,
            "success": success,
            "execution_time": execution_time,
            "timestamp": datetime.now().isoformat(),
            "notes": notes,
            "patterns": patterns or []
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É –≤ —Å–ø–∏—Å–æ–∫
        data["tasks"].append(task_record)

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á
        if len(data["tasks"]) > self.max_experience_tasks:
            data["tasks"] = data["tasks"][-self.max_experience_tasks:]

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        if patterns:
            for pattern in patterns:
                if pattern not in data["patterns"]:
                    data["patterns"][pattern] = []
                data["patterns"][pattern].append(task_id)

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        data["statistics"]["total_tasks"] = len(data["tasks"])
        data["statistics"]["successful_tasks"] = sum(1 for t in data["tasks"] if t["success"])
        data["statistics"]["failed_tasks"] = data["statistics"]["total_tasks"] - data["statistics"]["successful_tasks"]

        if execution_time and data["statistics"]["total_tasks"] > 0:
            # –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å)
            total_time = sum(t.get("execution_time", 0) for t in data["tasks"] if t.get("execution_time"))
            data["statistics"]["average_execution_time"] = total_time / data["statistics"]["total_tasks"]

        self._save_experience(data)

        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã –ø–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        if self.enable_indexing:
            self._update_indexes(task_record)

        # –û—á–∏—â–∞–µ–º –∫—ç—à, —Ç–∞–∫ –∫–∞–∫ –¥–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å
        self._clear_query_cache()

        return f"–û–ø—ã—Ç –∑–∞–¥–∞—á–∏ '{task_description}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω. –°—Ç–∞—Ç—É—Å: {'—É—Å–ø–µ—à–Ω–æ' if success else '–Ω–µ—É–¥–∞—á–Ω–æ'}"

    def _update_indexes(self, task_record: Dict[str, Any]) -> None:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏"""
        if not self.enable_indexing:
            return

        task_id = task_record.get("task_id", "")
        description = normalize_unicode_text(task_record.get("description", ""))
        patterns = task_record.get("patterns", [])

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
        words = set(description.split())
        for word in words:
            if word not in self._search_index:
                self._search_index[word] = set()
            self._search_index[word].add(task_id)

        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        for pattern in patterns:
            if pattern not in self._pattern_index:
                self._pattern_index[pattern] = []
            if task_id not in self._pattern_index[pattern]:
                self._pattern_index[pattern].append(task_id)

    def _get_query_hash(self, query: str, limit: int = 5) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ö—ç—à–∞ –¥–ª—è –∫–ª—é—á–∞ –∫—ç—à–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"""
        key_data = f"find_similar:{query}:{limit}"
        return hashlib.md5(key_data.encode('utf-8')).hexdigest()

    def _get_cache_entry(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –∫—ç—à–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π TTL"""
        if cache_key not in self._query_cache:
            self._cache_stats['misses'] += 1
            return None

        entry = self._query_cache[cache_key]
        cache_time = entry.get('timestamp', datetime.min)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º TTL
        if datetime.now() - cache_time >= timedelta(seconds=self.cache_ttl_seconds):
            # –ö—ç—à –ø—Ä–æ—Å—Ä–æ—á–µ–Ω, —É–¥–∞–ª—è–µ–º
            del self._query_cache[cache_key]
            self._cache_stats['evictions'] += 1
            self._cache_stats['misses'] += 1
            return None

        self._cache_stats['hits'] += 1
        return entry

    def _set_cache_entry(self, cache_key: str, result: List[Dict[str, Any]]) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –≤ –∫—ç—à–µ"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç —Ä–∞–∑–º–µ—Ä–∞ –∫—ç—à–∞
        if len(self._query_cache) >= self.cache_size:
            # –£–¥–∞–ª—è–µ–º —Å–∞–º—É—é —Å—Ç–∞—Ä—É—é –∑–∞–ø–∏—Å—å (–ø—Ä–æ—Å—Ç–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è LRU)
            oldest_key = min(self._query_cache.keys(),
                           key=lambda k: self._query_cache[k].get('timestamp', datetime.min))
            del self._query_cache[oldest_key]
            self._cache_stats['evictions'] += 1

        self._query_cache[cache_key] = {
            'result': result,
            'timestamp': datetime.now()
        }
        self._cache_stats['size'] = len(self._query_cache)

    def _clear_query_cache(self) -> None:
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"""
        self._query_cache.clear()
        self._cache_stats = {'hits': 0, 'misses': 0, 'evictions': 0, 'size': 0}

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Å—Ç–æ–π –∫—ç—à –Ω–∞ –¥–∏—Å–∫ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å
        if self.enable_cache_persistence:
            self._save_persistent_cache()

    def _load_persistent_cache(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–≥–æ –∫—ç—à–∞ —Å –¥–∏—Å–∫–∞"""
        cache_file_path = self.experience_dir / self.cache_file
        if not cache_file_path.exists():
            return

        try:
            with open(cache_file_path, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)

            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫—ç—à —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π TTL
            current_time = datetime.now()
            valid_entries = {}

            for key, entry in cache_data.get('query_cache', {}).items():
                entry_time = datetime.fromisoformat(entry.get('timestamp', ''))
                if current_time - entry_time < timedelta(seconds=self.cache_ttl_seconds):
                    valid_entries[key] = {
                        'result': entry.get('result', []),
                        'timestamp': entry_time
                    }

            self._query_cache = valid_entries
            self._cache_stats['size'] = len(self._query_cache)

            logger.debug(f"Loaded {len(self._query_cache)} valid cache entries from disk")

        except Exception as e:
            logger.warning(f"Failed to load persistent cache: {e}")

    def _save_persistent_cache(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–≥–æ –∫—ç—à–∞ –Ω–∞ –¥–∏—Å–∫"""
        if not self.enable_cache_persistence:
            return

        try:
            cache_file_path = self.experience_dir / self.cache_file
            cache_data = {
                'query_cache': self._query_cache,
                'stats': self._cache_stats,
                'metadata': {
                    'created_at': datetime.now().isoformat(),
                    'cache_size': self.cache_size,
                    'ttl_seconds': self.cache_ttl_seconds
                }
            }

            with open(cache_file_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False, default=str)

            logger.debug(f"Saved {len(self._query_cache)} cache entries to disk")

        except Exception as e:
            logger.error(f"Failed to save persistent cache: {e}")

    def _find_similar_tasks_uncached(self, query_normalized: str, limit: int = 5) -> List[Dict[str, Any]]:
        """–ù–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á"""
        data = self._load_experience()

        if self.enable_indexing and self._search_index:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            query_words = set(query_normalized.split())
            candidate_task_ids = set()

            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∑–∞–¥–∞—á–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–ª–æ–≤–æ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            for word in query_words:
                if word in self._search_index:
                    candidate_task_ids.update(self._search_index[word])

            # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–¥–∞—á–∏ –ø–æ —Ç–æ—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é –∑–∞–ø—Ä–æ—Å–∞
            similar_tasks = []
            for task in data["tasks"]:
                if task.get("task_id") in candidate_task_ids:
                    description_normalized = normalize_unicode_text(task["description"])
                    if query_normalized in description_normalized:
                        similar_tasks.append(task)
        else:
            # Fallback –¥–ª—è —Å–ª—É—á–∞–µ–≤ –±–µ–∑ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            similar_tasks = []
            for task in data["tasks"]:
                description_normalized = normalize_unicode_text(task["description"])
                if query_normalized in description_normalized:
                    similar_tasks.append(task)

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        return similar_tasks[-limit:]

    def find_similar_tasks(self, query: str, limit: int = 5) -> str:
        """
        –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á –≤ –∏—Å—Ç–æ—Ä–∏–∏

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á
        """
        query_normalized = normalize_unicode_text(query)
        cache_key = self._get_query_hash(query_normalized, limit)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_entry = self._get_cache_entry(cache_key)
        if cache_entry is not None:
            similar_tasks = cache_entry['result']
        else:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –∏ –∫—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            similar_tasks = self._find_similar_tasks_uncached(query_normalized, limit)
            self._set_cache_entry(cache_key, similar_tasks)

        if not similar_tasks:
            return f"–ü–æ—Ö–æ–∂–∏–µ –∑–∞–¥–∞—á–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'"

        result = f"–ù–∞–π–¥–µ–Ω–æ {len(similar_tasks)} –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á:\n\n"
        for i, task in enumerate(similar_tasks, 1):
            result += f"{i}. **{task['description']}**\n"
            result += f"   –°—Ç–∞—Ç—É—Å: {'‚úÖ –£—Å–ø–µ—à–Ω–æ' if task['success'] else '‚ùå –ù–µ—É–¥–∞—á–Ω–æ'}\n"
            execution_time = task.get('execution_time')
            if execution_time is not None:
                result += f"   –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time:.1f} —Å–µ–∫\n"
            if task.get('notes'):
                result += f"   –ó–∞–º–µ—Ç–∫–∏: {task['notes']}\n"
            result += "\n"

        return result

    def get_recommendations(self, current_task: str) -> str:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ–ø—ã—Ç–∞

        Args:
            current_task: –û–ø–∏—Å–∞–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –∑–∞–¥–∞—á–∏

        Returns:
            –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é
        """
        data = self._load_experience()

        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ —É—Å–ø–µ—à–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π Unicode –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        current_task_normalized = normalize_unicode_text(current_task)
        current_task_words = set(current_task_normalized.split())
        successful_similar = []
        for task in data["tasks"]:
            if task["success"]:
                description_normalized = normalize_unicode_text(task["description"])
                description_words = set(description_normalized.split())
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Å–ª–æ–≤–∞ —Ç–µ–∫—É—â–µ–π –∑–∞–¥–∞—á–∏ –µ—Å—Ç—å –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ –ø–æ—Ö–æ–∂–µ–π –∑–∞–¥–∞—á–∏
                if current_task_words.issubset(description_words):
                    successful_similar.append(task)

        if not successful_similar:
            return f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∑–∞–¥–∞—á–∏ '{current_task}': –î–∞–Ω–Ω—ã–µ –æ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á–∞—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–ª–µ–¥–æ–≤–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º –ø—Ä–∞–∫—Ç–∏–∫–∞–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏."

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω—ã–µ –∑–∞–¥–∞—á–∏
        recommendations = f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∑–∞–¥–∞—á–∏ '{current_task}' –Ω–∞ –æ—Å–Ω–æ–≤–µ {len(successful_similar)} —É—Å–ø–µ—à–Ω—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á:\n\n"

        # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        avg_time = sum(task.get("execution_time", 0) for task in successful_similar) / len(successful_similar)
        if avg_time > 0:
            recommendations += f"‚Ä¢ –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: ~{avg_time:.1f} —Å–µ–∫—É–Ω–¥\n"

        # –û–±—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        all_patterns = []
        for task in successful_similar:
            all_patterns.extend(task.get("patterns", []))

        if all_patterns:
            from collections import Counter
            pattern_counts = Counter(all_patterns)
            top_patterns = pattern_counts.most_common(3)
            if top_patterns:
                recommendations += "‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–µ—à–µ–Ω–∏—è:\n"
                for pattern, count in top_patterns:
                    recommendations += f"  - {pattern} (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è {count} —Ä–∞–∑)\n"

        # –ó–∞–º–µ—Ç–∫–∏ –∏–∑ —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–¥–∞—á
        useful_notes = [task["notes"] for task in successful_similar if task.get("notes")]
        if useful_notes:
            recommendations += "‚Ä¢ –ü–æ–ª–µ–∑–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–π:\n"
            for note in useful_notes[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 3 –∑–∞–º–µ—Ç–æ–∫
                recommendations += f"  - {note}\n"

        return recommendations

    def get_statistics(self) -> str:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è

        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
        """
        data = self._load_experience()
        stats = data.get("statistics", {})

        result = "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ LearningTool:\n\n"
        result += f"‚Ä¢ –í—Å–µ–≥–æ –∑–∞–¥–∞—á: {stats.get('total_tasks', 0)}\n"
        result += f"‚Ä¢ –£—Å–ø–µ—à–Ω—ã—Ö –∑–∞–¥–∞—á: {stats.get('successful_tasks', 0)}\n"
        result += f"‚Ä¢ –ù–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–¥–∞—á: {stats.get('failed_tasks', 0)}\n"

        if stats.get('average_execution_time', 0) > 0:
            result += f"‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {stats['average_execution_time']:.1f} —Å–µ–∫\n"

        patterns_count = len(data.get("patterns", {}))
        result += f"‚Ä¢ –ò–∑—É—á–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {patterns_count}\n"

        if stats.get('total_tasks', 0) > 0:
            success_rate = (stats.get('successful_tasks', 0) / stats['total_tasks']) * 100
            result += f"‚Ä¢ –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏: {success_rate:.1f}%\n"

        return result

    def get_cache_stats(self) -> str:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è

        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫—ç—à–∞
        """
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞ –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á
        total_requests = self._cache_stats['hits'] + self._cache_stats['misses']
        hit_rate = (self._cache_stats['hits'] / total_requests * 100) if total_requests > 0 else 0

        result = "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è LearningTool:\n\n"
        result += "**–ö—ç—à –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á:**\n"
        result += f"‚Ä¢ –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {total_requests}\n"
        result += f"‚Ä¢ –ü–æ–ø–∞–¥–∞–Ω–∏–π –≤ –∫—ç—à: {self._cache_stats['hits']}\n"
        result += f"‚Ä¢ –ü—Ä–æ–º–∞—Ö–æ–≤ –∫—ç—à–∞: {self._cache_stats['misses']}\n"
        result += f"‚Ä¢ –í—ã—Å–µ–ª–µ–Ω–∏–π –∏–∑ –∫—ç—à–∞: {self._cache_stats['evictions']}\n"
        result += f"‚Ä¢ –¢–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞: {self._cache_stats['size']}/{self.cache_size}\n"
        result += f"‚Ä¢ –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–ø–∞–¥–∞–Ω–∏–π: {hit_rate:.1f}%\n"
        result += f"‚Ä¢ TTL –∫—ç—à–∞: {self.cache_ttl_seconds} —Å–µ–∫\n"
        result += f"‚Ä¢ –ü–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∫—ç—à–∞: {'–≤–∫–ª—é—á–µ–Ω–∞' if self.enable_cache_persistence else '–æ—Ç–∫–ª—é—á–µ–Ω–∞'}\n"

        return result