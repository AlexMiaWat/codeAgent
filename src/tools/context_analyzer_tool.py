"""
ContextAnalyzerTool - –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ–µ–∫—Ç–∞
"""

import os
import re
import json
import logging
import unicodedata
from pathlib import Path
from typing import Dict, List, Any, Optional, Set
from crewai.tools.base_tool import BaseTool

logger = logging.getLogger(__name__)


def normalize_unicode_text(text: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç Unicode —Ç–µ–∫—Å—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç

    Returns:
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if not text:
        return text

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º Unicode (NFD - canonical decomposition)
    normalized = unicodedata.normalize('NFD', text)

    # –£–¥–∞–ª—è–µ–º –¥–∏–∞–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞–∫–∏ (combining characters)
    normalized = ''.join(char for char in normalized if unicodedata.category(char) != 'Mn')

    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    normalized = normalized.lower()

    return normalized


class ContextAnalyzerTool(BaseTool):
    """
    –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ–µ–∫—Ç–∞.

    –ü–æ–∑–≤–æ–ª—è–µ—Ç:
    - –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞
    - –ü–æ–Ω–∏–º–∞—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
    - –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –∑–∞–¥–∞—á
    """

    name: str = "ContextAnalyzerTool"
    description: str = """
    –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ–µ–∫—Ç–∞.
    –ü–æ–∑–≤–æ–ª—è–µ—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.
    """
    project_dir: str = "."
    docs_dir: str = "docs"
    max_file_size: int = 1000000
    supported_extensions: list = [".md", ".txt", ".rst", ".py", ".js", ".ts", ".json", ".yaml", ".yml"]

    def __init__(self, project_dir: str = ".", docs_dir: str = "docs",
                 max_file_size: int = 1000000, supported_extensions: List[str] = None, **kwargs):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ContextAnalyzerTool

        Args:
            project_dir: –ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞
            docs_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π
            max_file_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            supported_extensions: –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        """
        super().__init__(**kwargs)
        self.project_dir = Path(project_dir)
        self.docs_dir = self.project_dir / docs_dir
        self.max_file_size = max_file_size
        self.supported_extensions = supported_extensions or [".md", ".txt", ".rst", ".py", ".js", ".ts", ".json", ".yaml", ".yml"]

        # –ö—ç—à –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self._analysis_cache: Dict[str, Any] = {}
        self._dependency_cache: Dict[str, Set[str]] = {}

    def _run(self, action: str, **kwargs) -> str:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

        Args:
            action: –î–µ–π—Å—Ç–≤–∏–µ (analyze_project, find_dependencies, get_context, etc.)
            **kwargs: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ–π—Å—Ç–≤–∏—è

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏—è
        """
        try:
            if action == "analyze_project":
                return self.analyze_project_structure()
            elif action == "find_dependencies":
                return self.find_file_dependencies(kwargs.get("file_path", ""))
            elif action == "get_context":
                return self.get_task_context(kwargs.get("task_description", ""))
            elif action == "analyze_component":
                return self.analyze_component(kwargs.get("component_path", ""))
            elif action == "find_related_files":
                return self.find_related_files(kwargs.get("query", ""))
            else:
                return f"Unknown action: {action}"

        except Exception as e:
            logger.error(f"Error in ContextAnalyzerTool._run: {e}")
            return f"Error: {str(e)}"

    def analyze_project_structure(self) -> str:
        """
        –ê–Ω–∞–ª–∏–∑ –æ–±—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞

        Returns:
            –û–ø–∏—Å–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞
        """
        try:
            structure = {
                "directories": {},
                "file_types": {},
                "main_components": []
            }

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            for dir_path in self.project_dir.rglob("*"):
                if dir_path.is_dir() and not any(part.startswith('.') for part in dir_path.parts):
                    level = len(dir_path.relative_to(self.project_dir).parts)
                    if level <= 3:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≥–ª—É–±–∏–Ω—É –∞–Ω–∞–ª–∏–∑–∞
                        try:
                            file_count = len(list(dir_path.glob("*")))
                            structure["directories"][str(dir_path.relative_to(self.project_dir))] = file_count
                        except PermissionError:
                            continue

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥)
            file_counts = {}
            for file_path in self.project_dir.rglob("*"):
                if file_path.is_file():
                    ext = file_path.suffix.lower()
                    if ext in self.supported_extensions:
                        file_counts[ext] = file_counts.get(ext, 0) + 1

            structure["file_types"] = file_counts

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            main_dirs = ["src", "docs", "test", "config", "scripts"]
            for main_dir in main_dirs:
                dir_path = self.project_dir / main_dir
                if dir_path.exists():
                    structure["main_components"].append({
                        "name": main_dir,
                        "path": main_dir,
                        "files": len(list(dir_path.rglob("*.*")))
                    })

            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
            result = "üèóÔ∏è –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞:\n\n"

            result += "**–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**\n"
            for comp in structure["main_components"]:
                result += f"‚Ä¢ {comp['name']}: {comp['files']} —Ñ–∞–π–ª–æ–≤\n"

            result += "\n**–¢–∏–ø—ã —Ñ–∞–π–ª–æ–≤:**\n"
            for ext, count in structure["file_types"].items():
                result += f"‚Ä¢ {ext}: {count} —Ñ–∞–π–ª–æ–≤\n"

            result += "\n**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π:**\n"
            for dir_name, file_count in list(structure["directories"].items())[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥
                result += f"‚Ä¢ {dir_name}: {file_count} —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n"

            return result

        except Exception as e:
            logger.error(f"Error analyzing project structure: {e}")
            return f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞: {str(e)}"

    def find_file_dependencies(self, file_path: str) -> str:
        """
        –ü–æ–∏—Å–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Ñ–∞–π–ª–∞

        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É

        Returns:
            –°–ø–∏—Å–æ–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Ñ–∞–π–ª–∞
        """
        try:
            target_file = Path(file_path)
            if not target_file.is_absolute():
                target_file = self.project_dir / file_path

            if not target_file.exists():
                return f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}"

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            if target_file.stat().st_size > self.max_file_size:
                return f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {target_file.stat().st_size} –±–∞–π—Ç"

            dependencies = set()

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–º–ø–æ—Ä—Ç—ã –≤ Python —Ñ–∞–π–ª–∞—Ö
            if target_file.suffix == ".py":
                try:
                    with open(target_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # –ù–∞—Ö–æ–¥–∏–º –∏–º–ø–æ—Ä—Ç—ã
                    import_patterns = [
                        r'^import\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)',
                        r'^from\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)\s+import'
                    ]

                    for pattern in import_patterns:
                        matches = re.findall(pattern, content, re.MULTILINE)
                        for match in matches:
                            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
                            module_path = match.replace('.', '/')
                            possible_files = [
                                f"{module_path}.py",
                                f"{module_path}/__init__.py"
                            ]

                            for possible_file in possible_files:
                                if (self.project_dir / possible_file).exists():
                                    dependencies.add(possible_file)

                except Exception as e:
                    logger.error(f"Error analyzing Python imports: {e}")

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ –≤ Markdown —Ñ–∞–π–ª–∞—Ö
            elif target_file.suffix == ".md":
                try:
                    with open(target_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # –ù–∞—Ö–æ–¥–∏–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∞–π–ª—ã
                    link_patterns = [
                        r'\[([^\]]+)\]\(([^)]+\.md)\)',  # Markdown —Å—Å—ã–ª–∫–∏
                        r'\[([^\]]+)\]\(([^)]+\.py)\)',  # –°—Å—ã–ª–∫–∏ –Ω–∞ Python —Ñ–∞–π–ª—ã
                        r'([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*\.py)',  # Python —Ñ–∞–π–ª—ã –≤ —Ç–µ–∫—Å—Ç–µ
                    ]

                    for pattern in link_patterns:
                        matches = re.findall(pattern, content)
                        for match in matches:
                            if isinstance(match, tuple):
                                file_ref = match[1]
                            else:
                                file_ref = match

                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
                            ref_path = self.project_dir / file_ref
                            if ref_path.exists():
                                dependencies.add(file_ref)

                except Exception as e:
                    logger.error(f"Error analyzing Markdown links: {e}")

            if not dependencies:
                return f"–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è —Ñ–∞–π–ª–∞ {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

            result = f"üîó –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Ñ–∞–π–ª–∞ {file_path}:\n\n"
            for dep in sorted(dependencies):
                result += f"‚Ä¢ {dep}\n"

            return result

        except Exception as e:
            logger.error(f"Error finding dependencies: {e}")
            return f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π: {str(e)}"

    def get_task_context(self, task_description: str) -> str:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∑–∞–¥–∞—á–∏

        Args:
            task_description: –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏

        Returns:
            –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        """
        try:
            context_info = {
                "relevant_files": [],
                "documentation": [],
                "similar_tasks": [],
                "project_patterns": []
            }

            # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π Unicode –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
            task_normalized = normalize_unicode_text(task_description)

            # –ü–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            if self.docs_dir.exists():
                for doc_file in self.docs_dir.rglob("*.md"):
                    try:
                        if doc_file.stat().st_size > self.max_file_size:
                            continue

                        with open(doc_file, 'r', encoding='utf-8') as f:
                            content = f.read()

                        content_normalized = normalize_unicode_text(content)

                        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å —É—á–µ—Ç–æ–º Unicode –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                        if any(normalize_unicode_text(keyword) in content_normalized
                               for keyword in task_description.split()):
                            rel_path = doc_file.relative_to(self.project_dir)
                            context_info["documentation"].append(str(rel_path))

                    except UnicodeDecodeError:
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
                        continue
                    except Exception as e:
                        continue

            # –ü–æ–∏—Å–∫ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ
            src_dir = self.project_dir / "src"
            if src_dir.exists():
                for src_file in src_dir.rglob("*.py"):
                    try:
                        if src_file.stat().st_size > self.max_file_size:
                            continue

                        with open(src_file, 'r', encoding='utf-8') as f:
                            content = f.read()

                        content_normalized = normalize_unicode_text(content)

                        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å —É—á–µ—Ç–æ–º Unicode –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                        if any(normalize_unicode_text(keyword) in content_normalized
                               for keyword in task_description.split()):
                            rel_path = src_file.relative_to(self.project_dir)
                            context_info["relevant_files"].append(str(rel_path))

                    except UnicodeDecodeError:
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
                        continue
                    except Exception as e:
                        continue

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = f"üìã –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∑–∞–¥–∞—á–∏: '{task_description}'\n\n"

            if context_info["documentation"]:
                result += "**–°–≤—è–∑–∞–Ω–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:**\n"
                for doc in context_info["documentation"][:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥
                    result += f"‚Ä¢ {doc}\n"
                result += "\n"

            if context_info["relevant_files"]:
                result += "**–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã –∫–æ–¥–∞:**\n"
                for file in context_info["relevant_files"][:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥
                    result += f"‚Ä¢ {file}\n"
                result += "\n"

            if not context_info["documentation"] and not context_info["relevant_files"]:
                result += "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏–∑—É—á–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –ø—Ä–æ–µ–∫—Ç–∞.\n"

            return result

        except Exception as e:
            logger.error(f"Error getting task context: {e}")
            return f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {str(e)}"

    def analyze_component(self, component_path: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –ø—Ä–æ–µ–∫—Ç–∞

        Args:
            component_path: –ü—É—Ç—å –∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É

        Returns:
            –ê–Ω–∞–ª–∏–∑ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
        """
        try:
            component = Path(component_path)
            if not component.is_absolute():
                component = self.project_dir / component_path

            if not component.exists():
                return f"–ö–æ–º–ø–æ–Ω–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {component_path}"

            analysis = {
                "type": "directory" if component.is_dir() else "file",
                "size": component.stat().st_size if component.is_file() else 0,
                "files": 0,
                "subdirs": 0,
                "languages": {},
                "dependencies": []
            }

            if component.is_dir():
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                all_files = list(component.rglob("*.*"))
                analysis["files"] = len([f for f in all_files if f.is_file()])
                analysis["subdirs"] = len([d for d in component.rglob("*") if d.is_dir() and d != component])

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
                for file in all_files[:50]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    ext = file.suffix.lower()
                    if ext in analysis["languages"]:
                        analysis["languages"][ext] += 1
                    else:
                        analysis["languages"][ext] = 1

            elif component.is_file():
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª
                analysis["language"] = component.suffix

                # –ò—â–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
                deps_result = self.find_file_dependencies(str(component))
                if "‚Ä¢" in deps_result:
                    deps_lines = deps_result.split("\n")[2:]  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    analysis["dependencies"] = [line.strip("‚Ä¢ ").strip() for line in deps_lines if line.strip()]

            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
            result = f"üîç –ê–Ω–∞–ª–∏–∑ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: {component_path}\n\n"

            result += f"**–¢–∏–ø:** {analysis['type']}\n"

            if analysis["type"] == "directory":
                result += f"**–§–∞–π–ª–æ–≤:** {analysis['files']}\n"
                result += f"**–ü–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π:** {analysis['subdirs']}\n"

                if analysis["languages"]:
                    result += "**–Ø–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è:**\n"
                    for lang, count in sorted(analysis["languages"].items(), key=lambda x: x[1], reverse=True):
                        result += f"‚Ä¢ {lang}: {count} —Ñ–∞–π–ª–æ–≤\n"
            else:
                result += f"**–†–∞–∑–º–µ—Ä:** {analysis['size']} –±–∞–π—Ç\n"
                result += f"**–Ø–∑—ã–∫:** {analysis.get('language', '–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')}\n"

                if analysis["dependencies"]:
                    result += "**–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:**\n"
                    for dep in analysis["dependencies"]:
                        result += f"‚Ä¢ {dep}\n"

            return result

        except Exception as e:
            logger.error(f"Error analyzing component: {e}")
            return f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: {str(e)}"

    def find_related_files(self, query: str) -> str:
        """
        –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∑–∞–ø—Ä–æ—Å–æ–º

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        """
        try:
            query_normalized = normalize_unicode_text(query)
            related_files = []

            # –ò—â–µ–º –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            if self.docs_dir.exists():
                for doc_file in self.docs_dir.rglob("*.md"):
                    try:
                        if doc_file.stat().st_size > self.max_file_size:
                            continue

                        with open(doc_file, 'r', encoding='utf-8') as f:
                            content = f.read()

                        content_normalized = normalize_unicode_text(content)

                        if query_normalized in content_normalized:
                            related_files.append({
                                "path": str(doc_file.relative_to(self.project_dir)),
                                "type": "documentation",
                                "matches": content_normalized.count(query_normalized)
                            })

                    except UnicodeDecodeError:
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
                        continue
                    except Exception as e:
                        continue

            # –ò—â–µ–º –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ
            src_dir = self.project_dir / "src"
            if src_dir.exists():
                for src_file in src_dir.rglob("*.py"):
                    try:
                        if src_file.stat().st_size > self.max_file_size:
                            continue

                        with open(src_file, 'r', encoding='utf-8') as f:
                            content = f.read()

                        content_normalized = normalize_unicode_text(content)

                        if query_normalized in content_normalized:
                            related_files.append({
                                "path": str(src_file.relative_to(self.project_dir)),
                                "type": "code",
                                "matches": content_normalized.count(query_normalized)
                            })

                    except UnicodeDecodeError:
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
                        continue
                    except Exception as e:
                        continue

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            related_files.sort(key=lambda x: x["matches"], reverse=True)
            related_files = related_files[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥

            if not related_files:
                return f"–§–∞–π–ª—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∑–∞–ø—Ä–æ—Å–æ–º '{query}', –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

            result = f"üìÅ –§–∞–π–ª—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∑–∞–ø—Ä–æ—Å–æ–º '{query}':\n\n"

            for file_info in related_files:
                result += f"‚Ä¢ **{file_info['path']}** ({file_info['type']}) - {file_info['matches']} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π\n"

            return result

        except Exception as e:
            logger.error(f"Error finding related files: {e}")
            return f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {str(e)}"