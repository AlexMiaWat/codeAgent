"""
ContextAnalyzerTool - –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ–µ–∫—Ç–∞
"""

import re
import logging
import unicodedata
from pathlib import Path
from typing import Dict, List, Any, Set
from crewai.tools.base_tool import BaseTool

logger = logging.getLogger(__name__)


def normalize_unicode_text(text: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç Unicode —Ç–µ–∫—Å—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç

    Returns:
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if text is None:
        return ""
    if not text:
        return text

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º Unicode (NFD - canonical decomposition)
    normalized = unicodedata.normalize('NFD', text)

    # –£–¥–∞–ª—è–µ–º –¥–∏–∞–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞–∫–∏ (combining characters)
    normalized = ''.join(char for char in normalized if unicodedata.category(char) != 'Mn')

    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    normalized = normalized.lower()

    return normalized


class ContextAnalyzerTool(BaseTool):
    """
    –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ–µ–∫—Ç–∞.

    –ü–æ–∑–≤–æ–ª—è–µ—Ç:
    - –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞
    - –ü–æ–Ω–∏–º–∞—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
    - –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –∑–∞–¥–∞—á
    """

    name: str = "ContextAnalyzerTool"
    description: str = """
    –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ–µ–∫—Ç–∞.
    –ü–æ–∑–≤–æ–ª—è–µ—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.
    """
    project_dir: str = "."
    docs_dir: str = "docs"
    max_file_size: int = 1000000
    supported_extensions: list = [".md", ".txt", ".rst", ".py", ".js", ".ts", ".json", ".yaml", ".yml", ".java", ".cpp", ".hpp", ".c", ".h"]
    supported_languages: list = ["python", "javascript", "typescript", "java", "cpp", "c"]
    max_dependency_depth: int = 5

    def __init__(self, project_dir: str = ".", docs_dir: str = "docs",
                 max_file_size: int = 1000000, supported_extensions: List[str] = None,
                 supported_languages: List[str] = None, max_dependency_depth: int = 5, **kwargs):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ContextAnalyzerTool

        Args:
            project_dir: –ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞
            docs_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π
            max_file_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            supported_extensions: –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
            supported_languages: –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —è–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
            max_dependency_depth: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        """
        super().__init__(**kwargs)
        self.project_dir = Path(project_dir)
        self.docs_dir = self.project_dir / docs_dir
        self.max_file_size = max_file_size
        self.supported_extensions = supported_extensions or self.supported_extensions
        self.supported_languages = supported_languages or self.supported_languages
        self.max_dependency_depth = max_dependency_depth

        # –ö—ç—à –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self._analysis_cache: Dict[str, Any] = {}
        self._dependency_cache: Dict[str, Set[str]] = {}

    def _run(self, action: str, **kwargs) -> str:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

        Args:
            action: –î–µ–π—Å—Ç–≤–∏–µ (analyze_project, find_dependencies, get_context, etc.)
            **kwargs: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ–π—Å—Ç–≤–∏—è

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏—è
        """
        try:
            if action == "analyze_project":
                return self.analyze_project_structure()
            elif action == "find_dependencies":
                return self.find_file_dependencies(kwargs.get("file_path", ""))
            elif action == "get_context":
                return self.get_task_context(kwargs.get("task_description", ""))
            elif action == "analyze_component":
                return self.analyze_component(kwargs.get("component_path", ""))
            elif action == "find_related_files":
                return self.find_related_files(kwargs.get("query", ""))
            else:
                return f"Unknown action: {action}"

        except Exception as e:
            logger.error(f"Error in ContextAnalyzerTool._run with action '{action}': {e}", exc_info=True)
            return f"Error executing action '{action}': {str(e)}"

    def analyze_project_structure(self) -> str:
        """
        –ê–Ω–∞–ª–∏–∑ –æ–±—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞

        Returns:
            –û–ø–∏—Å–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            cache_key = "project_structure"
            if cache_key in self._analysis_cache:
                return self._analysis_cache[cache_key]
            structure = {
                "directories": {},
                "file_types": {},
                "main_components": []
            }

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            for dir_path in self.project_dir.rglob("*"):
                if dir_path.is_dir() and not any(part.startswith('.') for part in dir_path.parts):
                    level = len(dir_path.relative_to(self.project_dir).parts)
                    if level <= 3:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≥–ª—É–±–∏–Ω—É –∞–Ω–∞–ª–∏–∑–∞
                        try:
                            file_count = len(list(dir_path.glob("*")))
                            structure["directories"][str(dir_path.relative_to(self.project_dir))] = file_count
                        except PermissionError:
                            continue

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥)
            file_counts = {}
            for file_path in self.project_dir.rglob("*"):
                if file_path.is_file():
                    ext = file_path.suffix.lower()
                    if ext in self.supported_extensions:
                        file_counts[ext] = file_counts.get(ext, 0) + 1

            structure["file_types"] = file_counts

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            main_dirs = ["src", "docs", "test", "config", "scripts"]
            for main_dir in main_dirs:
                dir_path = self.project_dir / main_dir
                if dir_path.exists():
                    structure["main_components"].append({
                        "name": main_dir,
                        "path": main_dir,
                        "files": len(list(dir_path.rglob("*.*")))
                    })

            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
            result = "üèóÔ∏è –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞:\n\n"

            result += "**–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**\n"
            for comp in structure["main_components"]:
                result += f"‚Ä¢ {comp['name']}: {comp['files']} —Ñ–∞–π–ª–æ–≤\n"

            result += "\n**–¢–∏–ø—ã —Ñ–∞–π–ª–æ–≤:**\n"
            for ext, count in structure["file_types"].items():
                result += f"‚Ä¢ {ext}: {count} —Ñ–∞–π–ª–æ–≤\n"

            result += "\n**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π:**\n"
            for dir_name, file_count in list(structure["directories"].items())[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥
                result += f"‚Ä¢ {dir_name}: {file_count} —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n"

            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self._analysis_cache[cache_key] = result

            return result

        except Exception as e:
            logger.error(f"Error analyzing project structure for {self.project_dir}: {e}", exc_info=True)
            return f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞: {str(e)}"

    def find_file_dependencies(self, file_path: str) -> str:
        """
        –ü–æ–∏—Å–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Ñ–∞–π–ª–∞

        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É

        Returns:
            –°–ø–∏—Å–æ–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Ñ–∞–π–ª–∞
        """
        try:
            target_file = Path(file_path)
            if not target_file.is_absolute():
                target_file = self.project_dir / file_path

            if not target_file.exists():
                return f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}"

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            if target_file.stat().st_size > self.max_file_size:
                return f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {target_file.stat().st_size} –±–∞–π—Ç"

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            cache_key = str(target_file.relative_to(self.project_dir))
            if cache_key in self._dependency_cache:
                dependencies = self._dependency_cache[cache_key]
            else:
                dependencies = set()

                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
                file_extension = target_file.suffix.lower()
                file_name = target_file.name.lower()

                try:
                    with open(target_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # Python —Ñ–∞–π–ª—ã
                    if file_extension == ".py":
                        dependencies.update(self._analyze_python_dependencies(content))

                    # JavaScript/TypeScript —Ñ–∞–π–ª—ã
                    elif file_extension in [".js", ".ts", ".jsx", ".tsx"]:
                        dependencies.update(self._analyze_js_ts_dependencies(content))

                    # Java —Ñ–∞–π–ª—ã
                    elif file_extension == ".java":
                        dependencies.update(self._analyze_java_dependencies(content))

                    # C/C++ —Ñ–∞–π–ª—ã
                    elif file_extension in [".cpp", ".hpp", ".c", ".h"]:
                        dependencies.update(self._analyze_cpp_dependencies(content))

                    # Markdown —Ñ–∞–π–ª—ã
                    elif file_extension == ".md":
                        dependencies.update(self._analyze_markdown_links(content))

                    # YAML/JSON –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                    elif file_extension in [".yaml", ".yml", ".json"]:
                        if file_name == "pyproject.toml":
                            dependencies.update(self._analyze_pyproject_dependencies(content))
                        elif file_name == "package.json":
                            dependencies.update(self._analyze_package_json_dependencies(content))
                        else:
                            dependencies.update(self._analyze_config_dependencies(content, file_extension))

                    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
                    elif file_name == "requirements.txt":
                        dependencies.update(self._analyze_requirements_txt_dependencies(content))
                    elif file_name == "pyproject.toml":
                        dependencies.update(self._analyze_pyproject_dependencies(content))
                    elif file_name == "package.json":
                        dependencies.update(self._analyze_package_json_dependencies(content))

                    # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    self._dependency_cache[cache_key] = dependencies

                except UnicodeDecodeError:
                    logger.warning(f"Cannot decode file {target_file} with UTF-8 encoding")
                except Exception as e:
                    logger.error(f"Error analyzing dependencies for {target_file}: {e}")

            if not dependencies:
                return f"–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è —Ñ–∞–π–ª–∞ {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

            result = f"üîó –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Ñ–∞–π–ª–∞ {file_path}:\n\n"
            for dep in sorted(dependencies):
                result += f"‚Ä¢ {dep}\n"

            return result

        except Exception as e:
            logger.error(f"Error finding dependencies for file '{file_path}': {e}", exc_info=True)
            return f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π: {str(e)}"

    def _analyze_python_dependencies(self, content: str) -> Set[str]:
        """–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π Python —Ñ–∞–π–ª–æ–≤"""
        dependencies = set()

        # –ù–∞—Ö–æ–¥–∏–º –∏–º–ø–æ—Ä—Ç—ã
        import_patterns = [
            r'^import\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)',
            r'^from\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)\s+import'
        ]

        for pattern in import_patterns:
            matches = re.findall(pattern, content, re.MULTILINE)
            for match in matches:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
                module_path = match.replace('.', '/')
                possible_files = [
                    f"{module_path}.py",
                    f"{module_path}/__init__.py"
                ]

                for possible_file in possible_files:
                    if (self.project_dir / possible_file).exists():
                        dependencies.add(possible_file)

        return dependencies

    def _analyze_js_ts_dependencies(self, content: str) -> Set[str]:
        """–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π JavaScript/TypeScript —Ñ–∞–π–ª–æ–≤"""
        dependencies = set()

        # –ò–º–ø–æ—Ä—Ç—ã ES6
        import_patterns = [
            r'import\s+.*?\s+from\s+[\'"]([^\'"]+)[\'"]',
            r'import\s*\(\s*[\'"]([^\'"]+)[\'"]\s*\)',
            r'require\s*\(\s*[\'"]([^\'"]+)[\'"]\s*\)'
        ]

        for pattern in import_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
                if not match.startswith('.'):
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏
                if match.endswith('/'):
                    match = match[:-1]

                possible_extensions = ['.js', '.ts', '.jsx', '.tsx', '/index.js', '/index.ts']
                for ext in possible_extensions:
                    dep_path = match + ext
                    if (self.project_dir / dep_path).exists():
                        dependencies.add(dep_path)
                        break

        return dependencies

    def _analyze_java_dependencies(self, content: str) -> Set[str]:
        """–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π Java —Ñ–∞–π–ª–æ–≤"""
        dependencies = set()

        # –ò–º–ø–æ—Ä—Ç—ã –∫–ª–∞—Å—Å–æ–≤
        import_pattern = r'^import\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)\s*;'
        matches = re.findall(import_pattern, content, re.MULTILINE)

        for match in matches:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            class_path = match.replace('.', '/') + '.java'
            if (self.project_dir / class_path).exists():
                dependencies.add(class_path)

        return dependencies

    def _analyze_cpp_dependencies(self, content: str) -> Set[str]:
        """–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π C/C++ —Ñ–∞–π–ª–æ–≤"""
        dependencies = set()

        # Include –¥–∏—Ä–µ–∫—Ç–∏–≤—ã
        include_pattern = r'#include\s+["<]([^">]+)[">]'
        matches = re.findall(include_pattern, content)

        for match in matches:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ include —Ñ–∞–π–ª—ã
            if match.startswith('.'):
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ include

            possible_files = [match]
            if not match.endswith('.h') and not match.endswith('.hpp'):
                possible_files.extend([match + '.h', match + '.hpp'])

            for dep_file in possible_files:
                if (self.project_dir / dep_file).exists():
                    dependencies.add(dep_file)
                    break

        return dependencies

    def _analyze_markdown_links(self, content: str) -> Set[str]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫ –≤ Markdown —Ñ–∞–π–ª–∞—Ö"""
        dependencies = set()

        # –ù–∞—Ö–æ–¥–∏–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∞–π–ª—ã
        link_patterns = [
            r'\[([^\]]+)\]\(([^)]+\.md)\)',  # Markdown —Å—Å—ã–ª–∫–∏
            r'\[([^\]]+)\]\(([^)]+\.py)\)',  # –°—Å—ã–ª–∫–∏ –Ω–∞ Python —Ñ–∞–π–ª—ã
            r'\[([^\]]+)\]\(([^)]+\.(?:js|ts|java|cpp|h|hpp))\)',  # –°—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–¥
            r'([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*\.(?:py|js|ts|java|cpp|h|hpp))',  # –§–∞–π–ª—ã –≤ —Ç–µ–∫—Å—Ç–µ
        ]

        for pattern in link_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if isinstance(match, tuple):
                    file_ref = match[1]
                else:
                    file_ref = match

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
                ref_path = self.project_dir / file_ref
                if ref_path.exists():
                    dependencies.add(file_ref)

        return dependencies

    def _analyze_requirements_txt_dependencies(self, content: str) -> Set[str]:
        """–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏–∑ requirements.txt"""
        dependencies = set()

        for line in content.split('\n'):
            line = line.strip()
            if line and not line.startswith('#'):
                # –£–¥–∞–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –≤–µ—Ä—Å–∏–∏
                dep = line.split('#')[0].strip()
                dep = dep.split(';')[0].strip()  # –£–¥–∞–ª—è–µ–º —É—Å–ª–æ–≤–∏—è
                dep = dep.split('>=')[0].strip()
                dep = dep.split('==')[0].strip()
                dep = dep.split('<')[0].strip()
                dep = dep.split('>')[0].strip()
                dep = dep.split('!')[0].strip()

                if dep and dep != line:  # –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å
                    dependencies.add(dep)

        return dependencies

    def _analyze_pyproject_dependencies(self, content: str) -> Set[str]:
        """–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏–∑ pyproject.toml"""
        dependencies = set()

        try:
            import tomllib
            data = tomllib.loads(content)
        except ImportError:
            # –î–ª—è Python < 3.11 –∏—Å–ø–æ–ª—å–∑—É–µ–º tomli
            try:
                import tomli as tomllib
                data = tomllib.loads(content)
            except ImportError:
                logger.warning("tomli/tomllib not available for pyproject.toml parsing")
                return dependencies

        # –ò—â–µ–º —Å–µ–∫—Ü–∏–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        dependency_sections = [
            "project.dependencies",
            "project.optional-dependencies",
            "tool.poetry.dependencies",
            "tool.poetry.dev-dependencies",
            "build-system.requires"
        ]

        def extract_deps(obj):
            """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
            if isinstance(obj, dict):
                for key, value in obj.items():
                    if isinstance(value, str):
                        # –£–¥–∞–ª—è–µ–º –≤–µ—Ä—Å–∏–∏ –∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
                        dep = value.split('>=')[0].split('==')[0].split('<')[0].split('>')[0].split(';')[0].strip()
                        if dep and not dep.startswith('python'):
                            dependencies.add(dep)
                    elif isinstance(value, list):
                        for item in value:
                            extract_deps(item)
                    elif isinstance(value, dict):
                        extract_deps(value)
            elif isinstance(obj, list):
                for item in obj:
                    if isinstance(item, str):
                        dep = item.split('>=')[0].split('==')[0].split('<')[0].split('>')[0].split(';')[0].strip()
                        if dep and not dep.startswith('python'):
                            dependencies.add(dep)
                    elif isinstance(item, dict):
                        extract_deps(item)

        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º —Å–µ–∫—Ü–∏—è–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        for section_path in dependency_sections:
            current_data = data
            try:
                for part in section_path.split('.'):
                    current_data = current_data[part]
                extract_deps(current_data)
            except (KeyError, TypeError):
                continue

        return dependencies

    def _analyze_package_json_dependencies(self, content: str) -> Set[str]:
        """–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏–∑ package.json"""
        dependencies = set()

        try:
            import json
            data = json.loads(content)

            # –ò—â–µ–º –≤—Å–µ —Å–µ–∫—Ü–∏–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            dep_sections = [
                "dependencies",
                "devDependencies",
                "peerDependencies",
                "optionalDependencies",
                "bundledDependencies"
            ]

            for section in dep_sections:
                if section in data and isinstance(data[section], dict):
                    for dep_name in data[section].keys():
                        dependencies.add(dep_name)

        except (json.JSONDecodeError, KeyError) as e:
            logger.warning(f"Error parsing package.json: {e}")

        return dependencies

    def _analyze_config_dependencies(self, content: str, file_extension: str) -> Set[str]:
        """–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö"""
        dependencies = set()

        try:
            if file_extension == ".json":
                import json
                data = json.loads(content)
            else:  # YAML
                import yaml
                data = yaml.safe_load(content)

            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏—â–µ–º —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –ø—É—Ç—è–º–∏ –∫ —Ñ–∞–π–ª–∞–º
            def find_file_paths(obj, path=""):
                if isinstance(obj, dict):
                    for key, value in obj.items():
                        find_file_paths(value, f"{path}.{key}" if path else key)
                elif isinstance(obj, list):
                    for i, item in enumerate(obj):
                        find_file_paths(item, f"{path}[{i}]")
                elif isinstance(obj, str):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤—ã–≥–ª—è–¥–∏—Ç –ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∫ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
                    if any(obj.endswith(ext) for ext in self.supported_extensions):
                        if (self.project_dir / obj).exists():
                            dependencies.add(obj)

            find_file_paths(data)

        except Exception as e:
            logger.debug(f"Could not parse config file: {e}")

        return dependencies

    def get_task_context(self, task_description: str) -> str:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∑–∞–¥–∞—á–∏

        Args:
            task_description: –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏

        Returns:
            –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        """
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if not task_description or not isinstance(task_description, str):
            raise ValueError("task_description must be a non-empty string")

        if len(task_description) > 10000:  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –¥–ª–∏–Ω—É
            raise ValueError("task_description is too long (max 10000 characters)")

        try:
            context_info = {
                "relevant_files": [],
                "documentation": [],
                "similar_tasks": [],
                "project_patterns": []
            }

            # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π Unicode –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
            task_normalized = normalize_unicode_text(task_description)

            # –ü–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            if self.docs_dir.exists():
                for doc_file in self.docs_dir.rglob("*.md"):
                    try:
                        if doc_file.stat().st_size > self.max_file_size:
                            continue

                        with open(doc_file, 'r', encoding='utf-8') as f:
                            content = f.read()

                        content_normalized = normalize_unicode_text(content)

                        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å —É—á–µ—Ç–æ–º Unicode –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                        if any(normalize_unicode_text(keyword) in content_normalized
                               for keyword in task_description.split()):
                            rel_path = doc_file.relative_to(self.project_dir)
                            context_info["documentation"].append(str(rel_path))

                    except UnicodeDecodeError:
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
                        continue
                    except Exception:
                        continue

            # –ü–æ–∏—Å–∫ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ
            src_dir = self.project_dir / "src"
            if src_dir.exists():
                for src_file in src_dir.rglob("*.py"):
                    try:
                        if src_file.stat().st_size > self.max_file_size:
                            continue

                        with open(src_file, 'r', encoding='utf-8') as f:
                            content = f.read()

                        content_normalized = normalize_unicode_text(content)

                        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å —É—á–µ—Ç–æ–º Unicode –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                        if any(normalize_unicode_text(keyword) in content_normalized
                               for keyword in task_description.split()):
                            rel_path = src_file.relative_to(self.project_dir)
                            context_info["relevant_files"].append(str(rel_path))

                    except UnicodeDecodeError:
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
                        continue
                    except Exception:
                        continue

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = f"üìã –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∑–∞–¥–∞—á–∏: '{task_description}'\n\n"

            if context_info["documentation"]:
                result += "**–°–≤—è–∑–∞–Ω–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:**\n"
                for doc in context_info["documentation"][:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥
                    result += f"‚Ä¢ {doc}\n"
                result += "\n"

            if context_info["relevant_files"]:
                result += "**–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã –∫–æ–¥–∞:**\n"
                for file in context_info["relevant_files"][:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥
                    result += f"‚Ä¢ {file}\n"
                result += "\n"

            if not context_info["documentation"] and not context_info["relevant_files"]:
                result += "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏–∑—É—á–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –ø—Ä–æ–µ–∫—Ç–∞.\n"

            return result

        except Exception as e:
            logger.error(f"Error getting task context for '{task_description[:50]}...': {e}", exc_info=True)
            return f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {str(e)}"

    def analyze_component(self, component_path: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –ø—Ä–æ–µ–∫—Ç–∞

        Args:
            component_path: –ü—É—Ç—å –∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É

        Returns:
            –ê–Ω–∞–ª–∏–∑ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
        """
        try:
            component = Path(component_path)
            if not component.is_absolute():
                component = self.project_dir / component_path

            if not component.exists():
                return f"–ö–æ–º–ø–æ–Ω–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {component_path}"

            analysis = {
                "type": "directory" if component.is_dir() else "file",
                "size": component.stat().st_size if component.is_file() else 0,
                "files": 0,
                "subdirs": 0,
                "languages": {},
                "dependencies": []
            }

            if component.is_dir():
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                all_files = list(component.rglob("*.*"))
                analysis["files"] = len([f for f in all_files if f.is_file()])
                analysis["subdirs"] = len([d for d in component.rglob("*") if d.is_dir() and d != component])

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
                for file in all_files[:50]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    ext = file.suffix.lower()
                    if ext in analysis["languages"]:
                        analysis["languages"][ext] += 1
                    else:
                        analysis["languages"][ext] = 1

            elif component.is_file():
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª
                analysis["language"] = component.suffix

                # –ò—â–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
                deps_result = self.find_file_dependencies(str(component))
                if "‚Ä¢" in deps_result:
                    deps_lines = deps_result.split("\n")[2:]  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    analysis["dependencies"] = [line.strip("‚Ä¢ ").strip() for line in deps_lines if line.strip()]

            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
            result = f"üîç –ê–Ω–∞–ª–∏–∑ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: {component_path}\n\n"

            result += f"**–¢–∏–ø:** {analysis['type']}\n"

            if analysis["type"] == "directory":
                result += f"**–§–∞–π–ª–æ–≤:** {analysis['files']}\n"
                result += f"**–ü–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π:** {analysis['subdirs']}\n"

                if analysis["languages"]:
                    result += "**–Ø–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è:**\n"
                    for lang, count in sorted(analysis["languages"].items(), key=lambda x: x[1], reverse=True):
                        result += f"‚Ä¢ {lang}: {count} —Ñ–∞–π–ª–æ–≤\n"
            else:
                result += f"**–†–∞–∑–º–µ—Ä:** {analysis['size']} –±–∞–π—Ç\n"
                result += f"**–Ø–∑—ã–∫:** {analysis.get('language', '–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')}\n"

                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Å —É—á–µ—Ç–æ–º –≥–ª—É–±–∏–Ω—ã
                deps_result = self.find_file_dependencies(str(component))
                if "‚Ä¢" in deps_result:
                    deps_lines = deps_result.split("\n")[2:]  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    analysis["dependencies"] = [line.strip("‚Ä¢ ").strip() for line in deps_lines if line.strip()]

                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (–≥–ª—É–±–∏–Ω–∞ 2)
                    if self.max_dependency_depth > 1:
                        deep_deps = set()
                        for dep in analysis["dependencies"][:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                            dep_result = self.find_file_dependencies(dep)
                            if "‚Ä¢" in dep_result:
                                dep_lines = dep_result.split("\n")[2:]
                                for line in dep_lines:
                                    deep_dep = line.strip("‚Ä¢ ").strip()
                                    if deep_dep and deep_dep != dep:
                                        deep_deps.add(deep_dep)

                        if deep_deps:
                            analysis["deep_dependencies"] = list(deep_deps)[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥

                if analysis["dependencies"]:
                    result += "**–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:**\n"
                    for dep in analysis["dependencies"]:
                        result += f"‚Ä¢ {dep}\n"

                    if "deep_dependencies" in analysis and analysis["deep_dependencies"]:
                        result += "\n**–ì–ª—É–±–æ–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:**\n"
                        for dep in analysis["deep_dependencies"]:
                            result += f"  ‚Ä¢ {dep}\n"

            return result

        except Exception as e:
            logger.error(f"Error analyzing component: {e}")
            return f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: {str(e)}"

    def find_related_files(self, query: str) -> str:
        """
        –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∑–∞–ø—Ä–æ—Å–æ–º

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        """
        try:
            query_normalized = normalize_unicode_text(query)
            related_files = []

            # –ò—â–µ–º –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            if self.docs_dir.exists():
                for doc_file in self.docs_dir.rglob("*.md"):
                    try:
                        if doc_file.stat().st_size > self.max_file_size:
                            continue

                        with open(doc_file, 'r', encoding='utf-8') as f:
                            content = f.read()

                        content_normalized = normalize_unicode_text(content)

                        if query_normalized in content_normalized:
                            related_files.append({
                                "path": str(doc_file.relative_to(self.project_dir)),
                                "type": "documentation",
                                "matches": content_normalized.count(query_normalized)
                            })

                    except UnicodeDecodeError:
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
                        continue
                    except Exception:
                        continue

            # –ò—â–µ–º –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ
            src_dir = self.project_dir / "src"
            if src_dir.exists():
                for src_file in src_dir.rglob("*.py"):
                    try:
                        if src_file.stat().st_size > self.max_file_size:
                            continue

                        with open(src_file, 'r', encoding='utf-8') as f:
                            content = f.read()

                        content_normalized = normalize_unicode_text(content)

                        if query_normalized in content_normalized:
                            related_files.append({
                                "path": str(src_file.relative_to(self.project_dir)),
                                "type": "code",
                                "matches": content_normalized.count(query_normalized)
                            })

                    except UnicodeDecodeError:
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
                        continue
                    except Exception:
                        continue

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            related_files.sort(key=lambda x: x["matches"], reverse=True)
            related_files = related_files[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥

            if not related_files:
                return f"–§–∞–π–ª—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∑–∞–ø—Ä–æ—Å–æ–º '{query}', –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

            result = f"üìÅ –§–∞–π–ª—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∑–∞–ø—Ä–æ—Å–æ–º '{query}':\n\n"

            for file_info in related_files:
                result += f"‚Ä¢ **{file_info['path']}** ({file_info['type']}) - {file_info['matches']} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π\n"

            return result

        except Exception as e:
            logger.error(f"Error finding related files: {e}")
            return f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {str(e)}"